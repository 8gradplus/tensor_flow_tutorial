{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - some Theory\n",
    "For linear regression the closed solution is known via the so called normal equation\n",
    "Todo:\n",
    "1. Specify model\n",
    "2. Speficy loss function\n",
    "2. Write down log Likelihood function\n",
    "3. Minimize + get Normal equation\n",
    "\n",
    "### Approach 1: Non-probabilisitc using loss function\n",
    "The easies approach to linear regression is to use the squared loss function, \n",
    "$$l = \\sum_{i=1}^N (y_i - \\hat y_i)^2,$$\n",
    "where $\\hat y_i$ is the predicted value and $y_i$ is the true value of sample $i$. Together with a linear, non probabilistic model for the. For feature vector $x_i$ (where the first component is per convention the constant one -aka known as intercept) the prediction is given by the linear model. In scalar product notation:\n",
    "$$y_i =  x_i^T \\theta,$$\n",
    "with coefficient vector $\\theta$. Rewriting this in Matrix notation to account for all sampels we get for the loss together with the linear model:\n",
    "$$l = (Y - X\\theta)^T (Y - X\\theta)$$\n",
    "Now we would like to find $\\theta^*$ that _minimizes_ the loss. \n",
    "Setting the derivative w.r.t. $\\theta$ of the loss function zero  gives the normal equation \n",
    "$$X^T(Y - X\\theta) = 0.$$\n",
    "If $X^T X$ is non-singular (this is the case when there are more traning examples than features because then $X^T X$ is positive definite) allows for finding the unique solutions of the normal equation, \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "Note that this approach is non-probabilistic and thus, does not explicitly account for uncertainty  (as a probability distributions) in the data and coefficients.\n",
    "### Approach 2: Probabilistic  +  generative approach using max likelihood\n",
    "This approach introduces a probability distribution but does not explicitly consider a loss function. The  response is modelled  via a Normal distribution (\"Gauss error\") assuming constant standard deviation\n",
    "$$y_i = x_i^T \\theta + \\epsilon := \\mathcal N (x_i^T \\theta, \\sigma^2)$$\n",
    "In other words, the conditional distribtion  $p(y \\mid x, \\theta, \\sigma^2)$ is given by a Normal distribtion.\n",
    "The likelihood function is just the pdf of __all__ datapoints assuming i.i.d (this assumption in fact leads to the factorization), \n",
    "$$\\mathcal L = \\Pi_{i=1}^N p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "As we are aiming to optimize $\\theta$ in a way, a striclty monotonic transformation is applied on the likelihood function. It leaves the optimum invariant. The standard procedure is thus to consider the the logarithm ot likelihood function:\n",
    "$$\\mathcal L_l = \\sum_{i=1}^N \\log p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "Evaluating this expression for the Normal distribution gives\n",
    "$$\\mathcal L_l =  - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i - x_i^T \\theta)^2  - \\frac{N}{2}log(2\\pi\\sigma^2) $$\n",
    "Now, we would like to _maximise_ the likelihood and thus the log likelihood with respect to $\\theta$. This is equivalent to _minimizing_ the negative of it. Throwing away terms that, don't depend on $\\theta$ gives the function for which we would like to find the minimizer. That is we want to solve this expression, \n",
    "$$\\text{argmin}_\\theta\\left( Y - X \\theta \\right )^T \\left( Y - X \\theta \\right ),$$\n",
    "where we have rewritten the sum of squares over all training data again in Matrix notation. But this is exactly the same problem as in approach one and this gives the same solution (under the same circumstances), \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "**Remarks**\n",
    "* Note that this procedure only puts a probablilty distribution on the response $y$ while treating the remaining ingredients as variables (via the conditional pdf Ansatz). \n",
    "* This already implies that in this context the solutions $\\theta^*$ just tell how the pdf is parametrized (not even completely as we did not consider the optimum value of $\\sigma$).\n",
    "* In either case this approach does not really tell us how to predict a specific value $y$ for a given $x$. It just tells us the corresponding distribution of $y$. The fundamental reason is that we did not make any use of a loss function in this approach. \n",
    "* Pragmatically and in practice  of course, the prediction is made by plugging into the linear model as e.g. in the first appraoch\n",
    "* Note that a constant value for $\\sigma$ is called homoscedasticity. This implies that the variance does may not be a function of the features but only the mean within the Normal model for the response.\n",
    "* Furhtermore note that this approach does not consider uncertainties in the paramters. This would eventually require a Bayesian approach. \n",
    "\n",
    "### Approach 3: Probabilistic  +  discriminative approach using loss function and max likelihood\n",
    "\n",
    "### Apporach 4: A Bayesian approach\n",
    "\n",
    "Approaches: \n",
    "- Via max likelihood https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression\n",
    "- Minimize quadratic error directly\n",
    "\n",
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "Supervised = namedtuple(\"supervised\", [\"features\", \"target\", \"feature_names\"])\n",
    "\n",
    "def split_test_train(data):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data.features, data.target, test_size = 0.2, random_state=5)\n",
    "    train = Supervised(X_train, Y_train.reshape(-1, 1), feature_names=data.feature_names)\n",
    "    test = Supervised(X_test, Y_test.reshape(-1, 1), feature_names=data.feature_names)\n",
    "    return train, test \n",
    "\n",
    "\n",
    "def normalize(train: Supervised, test: Supervised):\n",
    "    mu = train.features.mean(axis=0)\n",
    "    std = train.features.std(axis=0)\n",
    "    train_scaled = Supervised(features=(train.features - mu) / std, \n",
    "                              target=train.target, \n",
    "                              feature_names=train.feature_names) \n",
    "    test_scaled = Supervised(features=(test.features - mu) / std, \n",
    "                             target=test.target, \n",
    "                             feature_names=test.feature_names)\n",
    "    return train_scaled, test_scaled\n",
    "\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "train_data, test_data = normalize(*split_test_train(Supervised(housing.data, housing.target, housing.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression via the Normal equation - with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(features):\n",
    "    \"\"\"Add intercept to features\n",
    "    Todo: as an exercise use tensorflow\"\"\"\n",
    "    m, n = features.shape\n",
    "    return np.c_[np.ones((m, 1)), features]\n",
    "\n",
    "    \n",
    "def train(data):\n",
    "    X = add_intercept(data.features)\n",
    "    Y = data.target\n",
    "    XT = tf.transpose(X)\n",
    "    return tf.matmul(tf.matmul(tf.linalg.inv(tf.matmul(XT, X)), XT), Y)\n",
    "\n",
    "\n",
    "def predict(data, theta):\n",
    "    return tf.matmul( add_intercept(data.features),  theta)\n",
    "\n",
    "theta_analytic = train(train_data)\n",
    "y_pred_analytic  = predict(test_data, theta_analytic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression as a Neural Network with Tensorflow / Keras \n",
    "* Idea: use keras wit 1 layer, identitiy as activation function\n",
    "* Assume that we could not find the minimizer analytically. \n",
    "* Thus we wish to find the minimzer computationally.\n",
    "\n",
    "> **Note** the accuracy with an explicit optimizier critically hinges on the convergence behaviour of the optimizer.  In this case wie use the Adam optimizer with learning rate = 0.01. It's instructive to break the model by employing a non-convergent optimizer (e.g. RSMprops with learnin rate 0.01 for this particular case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_keras_training(history):\n",
    "    plt.figure(figsize=(15,4))\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "    plotter.plot({'Basic': history}, metric = \"mse\")\n",
    "    plt.ylim([0, 3])\n",
    "    plt.xlim([0,30])\n",
    "    plt.ylabel('MSE')\n",
    "    plt.show()\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([layers.Dense(1, activation='linear', input_shape=[8])])\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:6.0507,  mse:6.0507,  val_loss:4.7999,  val_mse:4.7999,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.5212,  mse:0.5212,  val_loss:0.5390,  val_mse:0.5390,  \n",
      ".........................................................................................."
     ]
    }
   ],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "history = model.fit(\n",
    "  train_data.features, train_data.target,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
    "  callbacks=[tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_keras = tf.concat((model.weights[1], tf.reshape(model.weights[0], [8])), axis=0)\n",
    "y_pred_keras = model.predict(test_data.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using the estimator API\n",
    "* within the estimator API the number of training steps is controlled via the the `dataset`\n",
    "* more precisely via the `repeat` keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.estimator import LinearRegressor\n",
    "from tensorflow import feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data: Supervised, train=False):\n",
    "    \"\"\"Todo: make callable class for more intuitive usage lateron\"\"\"\n",
    "    raw_xs = pd.DataFrame(data.features, columns = data.feature_names)\n",
    "    xs = {key:np.array(value) for key,value in dict(raw_xs).items()} \n",
    "    ys = data.target\n",
    "    ds = tf.data.Dataset.from_tensor_slices((xs, ys)).batch(raw_xs.shape[0])\n",
    "    if train:\n",
    "        return ds.repeat(200)\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [feature_column.numeric_column(f) for f in housing.feature_names]\n",
    "linear_regressor = LinearRegressor(feature_columns=feature_columns)\n",
    "linear_regressor.train(lambda: input_fn(train_data, train=True))\n",
    "y_pred_estimator = np.array([r[\"predictions\"] for r in linear_regressor.predict(lambda: input_fn(test_data, train=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in linear_regressor.get_variable_names():\n",
    "    print(name, linear_regressor.get_variable_value(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison perform linear regression with scikit learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_model = LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_model.fit(train_data.features, train_data.target)\n",
    "theta_sklearn = tf.concat((lin_model.intercept_, lin_model.coef_.flatten()), axis=0)\n",
    "y_pred_sklearn = lin_model.predict(test_data.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return tf.reduce_sum((y_true - y_pred)**2) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(test_data.target,  y_pred_sklearn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(test_data.target,  y_pred_analytic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(test_data.target,  y_pred_keras) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(test_data.target,  y_pred_estimator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_estimator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
