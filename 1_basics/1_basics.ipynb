{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* https://github.com/ageron/handson-ml2\n",
    "\n",
    "https://www.tensorflow.org/guide/effective_tf2\n",
    "Use Python to define computational graph, which is exectued in C++ Code.\n",
    "Due to the graphical structure TF is highly parallizeable.\n",
    "TF can compute gradients automatically.\n",
    "The main python API is very flexible at the cost of higher complexity.\n",
    "It comes with tensorboard: a visualization tool for the computational graph.\n",
    "\n",
    "### Tensor flow APIs for machine learning\n",
    "* `tensorflow.estimator`: API for predefined models (very simalar to scikit-learn)\n",
    "* `tensorflow.losses`: Losses\n",
    "* `tensorflow.metrics`: Metrics\n",
    "* `tensorflow.layes`: Neural network layes\n",
    "\n",
    "\n",
    "\n",
    "### High level APIs to Tensorflow\n",
    "* todo\n",
    "\n",
    "### Rought Structure of Tensorflow\n",
    "In principle (as e.g., in Spark) a tensorflow computation consists of two steps:\n",
    "1. **Construction phase:**\n",
    " This parts specifies the computational graph. I.e., it specifies the model to compute\n",
    " In tensorflow we call the associated object _dataflow graph_ (or just graph). Any additional variable is just a node added to the graph. The individual nodes are **Tensors** \n",
    " and **operations**. Tensors don't carry any data. \n",
    " \n",
    "2. **Execution phase:**\n",
    "* This part then runs the actual computation and returns results. \n",
    "* In Tensorflow the associated object is called a _session_. The return dypes of a session are **numpy arrays**.\n",
    "\n",
    "\n",
    "**In TF 2 this has changed as eager exectution is put forward (so there is no more the need to work with the `session` context as in TF 1)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [Datatypes](https://www.tensorflow.org/tutorials/customization/basics)\n",
    "* Basic building block are `tf.Tensor`\n",
    "* Very similar to `np.ndarray`\n",
    "* However there are also differences\n",
    "    - are **immutable**\n",
    "    - _can_ live in accelerator memory: GPU and TPU (Tensor processing unit)\n",
    "    \n",
    "    \n",
    "### Operations on Tensors\n",
    "- a rich library over tensors https://www.tensorflow.org/guide/effective_tf2. In particular:\n",
    "- Tensor manipulation: masking, broadcasting, ...\n",
    "- https://www.tensorflow.org/api_docs/python/tf/function\n",
    "- [Linear algebra](https://www.tensorflow.org/api_docs/python/tf/linalg)\n",
    "- [Mathematical functions](https://www.tensorflow.org/api_docs/python/tf/math)\n",
    "- [Fourier tranformations](https://www.tensorflow.org/api_docs/python/tf/signal)\n",
    "- [Statistical operations](https://www.tensorflow.org/api_docs/python/tf/random)\n",
    "- [Sparse tensor representation](https://www.tensorflow.org/api_docs/python/tf/sparse)\n",
    "- [String operations](https://www.tensorflow.org/api_docs/python/tf/strings)\n",
    "- [But you can also make your costum ops in C++](https://www.tensorflow.org/guide/create_op)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "### Compatibility with numpy\n",
    "\n",
    "* `tf.Tensor` $\\rightarrow$ `np.ndarray` \n",
    "    - Numpy operations automatically convert Tensors to  NumPy ndarrays\n",
    "    - explicitly with the `numpy()` method on a Tensor\n",
    "* `tf.Tensor` $\\leftarrow$ `np.ndarray` \n",
    "    - TensorFlow operations automatically convert NumPy ndarrays to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6]\n",
      "[6 6]\n",
      "[[1 2 3]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array = np.array([[1,2,3], [1,2,3]])\n",
    "tensor = tf.constant(numpy_array)\n",
    "\n",
    "# implicit conversion of ndarray -> tensor within tf function\n",
    "tf.print(tf.math.reduce_sum(numpy_array, axis=1))\n",
    "\n",
    "# implicit conversion tensor -> ndarray within numpy function\n",
    "print(np.sum(tensor, axis=1)) \n",
    "\n",
    "# explicit conversion tensor -> nd.arrray\n",
    "print(tensor.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "> A Tensor is a symbolic handle to one of the outputs of an Operation. \n",
    "It does not hold the values of that operation's output, but instead provides a means of computing those values.\n",
    "A `tf.Tensor` object represents a partially defined computation that will eventually produce a value. TensorFlow programs work by first building a graph of tf.Tensor objects, detailing how each tensor is computed based on the other available tensors and then by running parts of this graph to achieve the desired results.\n",
    "\n",
    "A Tensor has always a \n",
    "* datatype\n",
    "* shape\n",
    "\n",
    "Tensors can be:\n",
    "* reshaped\n",
    "* sliced \n",
    "* cast to other datatypes\n",
    "* broadcasted\n",
    "* [see notebook](https://colab.research.google.com/notebooks/mlcc/creating_and_manipulating_tensors.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=tensors-colab&hl=de)\n",
    "\n",
    "There Are several types of tensors\n",
    "\n",
    "* `tf.Variable`\n",
    "* `tf.constant`\n",
    "* `tf.placeholder`\n",
    "* `tf.SparseTensor`\n",
    "\n",
    "* Apart from `tf.Variable`, Tensors are _immuatable_\n",
    "\n",
    "\n",
    "\n",
    "> If there's a tf.device scope active, the variable will be placed on that device; otherwise the variable will be placed on the \"fastest\" device compatible with its dtype (this means most variables are automatically placed on a GPU if one is available). For example, the following snippet creates a variable named v and places it on the second GPU device:\n",
    "\n",
    "\n",
    "~~~~(.python)\n",
    "with tf.device(\"/device:GPU:1\"):\n",
    "  v = tf.Variable(tf.zeros([10, 10]))\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 2), dtype=int16, numpy=\n",
       "array([[[10, 10],\n",
       "        [10, 10]],\n",
       "\n",
       "       [[10, 10],\n",
       "        [10, 10]]], dtype=int16)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = tf.constant([1,2,3], tf.float32)\n",
    "var = tf.Variable(vec)\n",
    "\n",
    "# variables are mutable: \n",
    "var.assign_add(tf.ones(3))\n",
    "\n",
    "# get the rank (=number of dimensions)\n",
    "tf.rank(vec)\n",
    "\n",
    "x = tf.ones([2,2,2], tf.int16)\n",
    "\n",
    "# reshape\n",
    "tf.reshape(x, [4,-1])\n",
    "\n",
    "# cast\n",
    "tf.cast(x, tf.float32)\n",
    "\n",
    "# slicing\n",
    "x[1,:,:] \n",
    "\n",
    "# broadcasting\n",
    "x * 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data input pipelines with [Data Sets](https://www.tensorflow.org/guide/data)\n",
    "* Build data pipelines to feed model\n",
    "* It consists of 3 Steps:\n",
    "    - Create _source_ dataset (i.e. reading slices from a tensor of from a text filed)\n",
    "    - Apply transformation (`map`, `shuffle`, `batch`) \n",
    "    - Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4 9], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1 25], shape=(2,), dtype=int32)\n",
      "tf.Tensor([36 16], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# create source dataset \n",
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "# apply transformations\n",
    "ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)\n",
    "# iterate over input data for model\n",
    "for t in ds_tensors:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizaton and slicing\n",
    "\n",
    "\n",
    "### GPU support\n",
    "\n",
    "\n",
    "### Performance\n",
    "* eager exectution vs performance\n",
    "* `tf.function`\n",
    "> In general, it's not necessary to decorate each of these smaller functions with tf.function; only use tf.function to decorate high-level computations - for example, one step of training or the forward pass of your model.\n",
    "\n",
    "# Tensorflow 1 specifics\n",
    "\n",
    "### Graphs\n",
    "* So far we did not assign any explicit graph\n",
    "* The reason is that tensoflow introduces the so called **default graph**\n",
    "* Any variable specified get's added to the default graph\n",
    "* However, we may also introduce other graphs, apart from the default graph.\n",
    "\n",
    "### Node Lifecycle\n",
    "* All node _values_ are dropped between graph runs (within a session).\n",
    "* Node values are (intermediate) results\n",
    "* This implies that node values are not reused (as in spark) per default.\n",
    "* This leads to  inefficiencies if also intermediate results are of interest. \n",
    "* For each (intermediate) result output TF runs an individual graph run.\n",
    "* In order to avoid redundant evaluations, you need to ask tensorflow to **evaluate all interesting variables within a single graph run**.\n",
    "* This can be achieved by \n",
    "\n",
    "\n",
    "\n",
    "#### Additional Information from the textbook (A. Geron, _Hands on machine learning with scikit-learn and tensoflow_, 2017):\n",
    "\n",
    "> All node values are dropped between graph runs, except variable values, which are maintained by the\n",
    "session across graph runs (queues and readers also maintain some state, as we will see in Chapter 12). A\n",
    "variable starts its life when its initializer is run, and it ends when the session is closed.\n",
    "\n",
    "> In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (each session would have its own copy of every variable). In distributed TensorFlow (see Chapter 12), variable state is stored on the servers, not in the sessions, so multiple sessions can share the same variables.\n",
    "\n",
    "\n",
    "# Todos / Brainstorming\n",
    "* TF2 has greatly improved name spacing of variables (https://www.tensorflow.org/guide/effective_tf2) \n",
    "* How to implement fit predict cleanly? \n",
    "* Warnings with Jupyter notebooks\n",
    "* Visualization of the computational graph\n",
    "\n",
    "### Possible Examples\n",
    "* Logistic Regression\n",
    "* Linear Regression\n",
    "* Generalized linear models\n",
    "* Survival Analysis\n",
    "* Customer Livetime values\n",
    "* Decision Tree\n",
    "* Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
