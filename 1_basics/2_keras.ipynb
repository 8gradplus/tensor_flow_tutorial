{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "* In principle losses can be defined manually using tensor datatypes. \n",
    "* However as it is so common to use losses in order to train a model via optimization (reducing loss), tensor flow provides the common loss functions\n",
    "* [`tf.losses`](https://www.tensorflow.org/api_docs/python/tf/losses)\n",
    "\n",
    "# Optimizers\n",
    "* Tensor flow strongly follows the neural nets paradigm\n",
    "- Provide training data (`tf.data` or `tf.placeholder`)\n",
    "- Set up model (`tf.layers`)\n",
    "- Set up loss (`tf.losses`)\n",
    "- Find model parameters by optimizing the loss evaluated on training data\n",
    "\n",
    "* What's the \"output\" of an optimizer? \n",
    "- a single step\n",
    "- iterations are controlled by `batch size` and `epochs` in `Keras`\n",
    "- iterations need to be controlled via `repeat` _within Datasets_ for `Estimators`\n",
    "- How to track optimization History for `Estimators`?\n",
    "\n",
    "* So the last ingredient are optimizers!\n",
    "- `tf.train`\n",
    "- The simplest optimizer one can imagine is gradient descent: `tf.train.GradientDescentOptimizer`\n",
    "- They have to be instantiated and thrown on a loss function (that in turn depends on input data).\n",
    "\n",
    "~~~~(.python)\n",
    "loss = ??? \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "~~~~\n",
    "\n",
    "\n",
    "\n",
    "# Keras\n",
    "* **3 Ways of constructing a model:** \n",
    "    - sequenital API \n",
    "    - functional API \n",
    "    - model subclassing APIs\n",
    "* **Fitting:**\n",
    "    - `fit()` method (native keras)\n",
    "    - `gradientTape` (tensorflow addon)\n",
    "    \n",
    "For all of the following properties Keras offers a rich familiy of built in implementations but also allows to write costum ones:    \n",
    "* Layers\n",
    "* Weight intializtion\n",
    "* Optimizers\n",
    "* Regularizers\n",
    "* Activation functions\n",
    "* Loss functions (= objective function for optimizers)\n",
    "* Metrics (for judging model performance)\n",
    "\n",
    "\n",
    "### Transfer learning\n",
    "\n",
    "### Regularization.\n",
    "At a layer the following structure $a(Wx + b)$, where $W$ are called weights and $b$ bias and $a$ ist the activation function. According to these 3 ingredients there are three types of [regularizations in Keras](https://keras.io/regularizers/):\n",
    "* Kernel regularizer: act on $W$\n",
    "* Bias regularizer: act on $b$\n",
    "* activation regularizer: act on (output of) $a$\n",
    "\n",
    "*Questions:*\n",
    "* How are these 3 regularizers combined\n",
    "\n",
    "\n",
    "### Keras Backend\n",
    "* Keras ist compatible with [various backends](https://keras.io/backend/) (tensorflow, theano, CNTK)\n",
    "* Mathematical calls via the backend ensure consistency. \n",
    "* This is mainly used for custom regularization functions or loss functions\n",
    "```\n",
    "import Keras.backend as K\n",
    "K.sum(x)\n",
    "```\n",
    "instead of the tensorflow specific call \n",
    "```\n",
    "tf.math.reduce_sum(x)\n",
    "```\n",
    "\n",
    "### Costum Layers\n",
    "* [Dokumentation](https://keras.io/layers/writing-your-own-keras-layers/)\n",
    "\n",
    "### Costum Regulariziations\n",
    "* [Dokumentation](https://keras.io/regularizers/)\n",
    "* Each of the 3 regularizers can be implemented manually and then passed to keras\n",
    "* How is a regularizer implemented that combines e.g. weights and biases in a non-additive fashion?\n",
    "* Signature for weight regularization\n",
    "`def regularizer(weight_matrix) -> float:`\n",
    "* Example\n",
    "\n",
    "```\n",
    "from keras import backend as K\n",
    "\n",
    "def l1_reg(weight_matrix):\n",
    "    return 0.01 * K.sum(K.abs(weight_matrix))\n",
    "\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=l1_reg))\n",
    "```\n",
    "And similar for bias and activation regularization functions. \n",
    "\n",
    "**Question:**\n",
    "How to write regularization functions that take e.g. both weights and biases as input and operate in non-addidtive fashion.\n",
    "\n",
    "\n",
    "### Custom Loss functions\n",
    "* Apart from the [available loss](https://keras.io/losses/) functions in Keras it is possible to construct costum ones\n",
    "* The have to obey the signature `def regularizer(y_true, y_pred) -> float:`\n",
    "* So it takes the true and the predicted labels as input.\n",
    "\n",
    "**Question:**\n",
    "How to write loss functions that also depend on the layer weights?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
