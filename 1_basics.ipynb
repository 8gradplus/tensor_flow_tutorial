{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python to define computational graph, which is exectued in C++ Code.\n",
    "Due to the graphical structure TF is highly parallizeable.\n",
    "TF can compute gradients automatically.\n",
    "The main python API is very flexible at the cost of higher complexity.\n",
    "It comes with tensorboard: a visualization tool for the computational graph.\n",
    "\n",
    "### High level APIs to Tensorflow\n",
    "* `tensorflow.contrib.learn`: API compatible with scikit-learn\n",
    "* `tensorflow.contrib.slim`: API to simplify computations\n",
    "* `tensorflow.contrib.keras`: Keras API\n",
    "\n",
    "### Rought Structure of Tensorflow\n",
    "In principle (as e.g., in Spark) a tensorflow computation consists of two steps:\n",
    "1. **Construction phase:**\n",
    " This parts specifies the computational graph. I.e., it specifies the model to compute\n",
    " In tensorflow we call the associated object _dataflow graph_ (or just graph). Any additional variable is just a node added to the graph.\n",
    " \n",
    "2. **Execution phase:**\n",
    "This part then runs the actual computation and returns results. \n",
    "In Tensorflow the associated object is called a _session_\n",
    "### Todos / Brainstorming\n",
    "* How to implement fit predict cleanly? \n",
    "* Warnings with Jupyter notebooks\n",
    "* Visualization of the computational graph\n",
    "\n",
    "### Possible Examples\n",
    "* Logistic Regression\n",
    "* Linear Regression\n",
    "* Generalized linear models\n",
    "* Survival Analysis\n",
    "* Customer Livetime values\n",
    "* Decision Tree\n",
    "* Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the computational graph\n",
    "# Neither functions are evaluated nor variables are initialized\n",
    "# Not executed yet (just like a tranformation in spark)\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(5, name=\"y\")\n",
    "f = x*x*x + x + y +42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "# A Session 1) initializes the variables + computes the graph\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer) # Initialize Variables\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f) # Run Graph\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "# Convientient alternative via context\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing variables\n",
    "**TF introduces shortcuts for readability:**\n",
    "* `x.initializer.run()` is equivalent to `tf.get_default_session().run(x.initializer)`\n",
    "* `f.eval()` is equivalent to `tf.get_default_session().run(f)` \n",
    "\n",
    "**Globally initializing variables**\n",
    "* Instead of  initializing each  variable individually, this may be done globally.\n",
    "* By calling `tf.global_variables_initializer()`\n",
    "* So the above code may be written more compactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() # add init node\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually run init\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow for experiments (jupyter notebook)\n",
    "* There are several ways to:\n",
    "    - speed up experimenting with TF\n",
    "    - reduce boilerplates (executing sessions)\n",
    "    - these are an 1) interactive tf session or 2) to enable eager execution (get rid of session calls)\n",
    "### Interactive TF session\n",
    "* Useful in particular for jupyter notebooks\n",
    "* Don't forget to close the session afterwards (would be done automatically within the session context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager execution\n",
    "* [Tensor flow documentation'](https://www.tensorflow.org/guide/eager)\n",
    "* Within this execution mode TF operates similar to numpy \n",
    "* I.e, the graph is directly evaluated and thus calling the execution phase immediatly\n",
    "* To this end we need to tell tensorflow\n",
    "    - `tf.enable_eager_execution()`\n",
    "    \n",
    "**However, this operational mode is still under development!**\n",
    "\n",
    "> Eager execution is not included in the latest release (version 1.4) of TensorFlow. To use it, you will need to build TensorFlow from source or install the nightly builds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow.contrib.eager as tfe\\ntfe.enable_eager_execution()\\nx = [[2.]]\\nm = tf.matmul(x, x)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()\n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs\n",
    "* So far we did not assign any explicit graph\n",
    "* The reason is that tensoflow introduces the so called **default graph**\n",
    "* Any variable specified get's added to the default graph\n",
    "* However, we may also introduce other graphs, apart from the default graph.\n",
    "\n",
    "#### Graphs within jupyter notebooks\n",
    "* As jupyter notebooks are used for experimenting and cells are executed various times, it is possible that the graph get's \"messed up\". \n",
    "* In order to clean the graph tensoflow allows to clean the graph by the command `tf.reset_default_graph()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is x1 in default graph?: True\n",
      "Is x2 is in my_graph?: True\n",
      "Is x2 is in default_graph?: False\n"
     ]
    }
   ],
   "source": [
    "# node is added to default graph\n",
    "x1 = tf.Variable(3, \"x1\")\n",
    "print(\"Is x1 in default graph?:\", x1.graph is tf.get_default_graph())\n",
    "\n",
    "# create a new graph object\n",
    "my_graph = tf.Graph()\n",
    "with my_graph.as_default():\n",
    "    x2 = tf.Variable(30, \"x2\")\n",
    "\n",
    "print(\"Is x2 is in my_graph?:\", x2.graph is my_graph) \n",
    "print(\"Is x2 is in default_graph?:\", x2.graph is tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is x3 is in default graph: False\n"
     ]
    }
   ],
   "source": [
    "x3 = tf.Variable(333, \"x3\") # add to default graph\n",
    "tf.reset_default_graph() # reset default graph\n",
    "print(\"Is x3 is in default graph:\", x3.graph is tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Lifecycle\n",
    "* All node _values_ are dropped between graph runs (within a session).\n",
    "* Node values are (intermediate) results\n",
    "* This implies that node values are not reused (as in spark) per default.\n",
    "* This leads to  inefficiencies if also intermediate results are of interest. \n",
    "* For each (intermediate) result output TF runs an individual graph run.\n",
    "* In order to avoid redundant evaluations, you need to ask tensorflow to **evaluate all interesting variables within a single graph run**.\n",
    "* This can be achieved by \n",
    "\n",
    "\n",
    "\n",
    "#### Additional Information from the textbook (A. Geron, _Hands on machine learning with scikit-learn and tensoflow_, 2017):\n",
    "\n",
    "> All node values are dropped between graph runs, except variable values, which are maintained by the\n",
    "session across graph runs (queues and readers also maintain some state, as we will see in Chapter 12). A\n",
    "variable starts its life when its initializer is run, and it ends when the session is closed.\n",
    "\n",
    "> In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (each session would have its own copy of every variable). In distributed TensorFlow (see Chapter 12), variable state is stored on the servers, not in the sessions, so multiple sessions can share the same variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "# Redundant graph evaluation: 2 graphs are evaluated\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # 10\n",
    "    print(z.eval()) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# one graph computation only\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val =  sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - some Theory\n",
    "For linear regression the closed solution is known via the so called normal equation\n",
    "Todo:\n",
    "1. Specify model\n",
    "2. Speficy loss function\n",
    "2. Write down log Likelihood function\n",
    "3. Minimize + get Normal equation\n",
    "\n",
    "### Approach 1: Non-probabilisitc using loss function\n",
    "The easies approach to linear regression is to use the squared loss function, \n",
    "$$l = \\sum_{i=1}^N (y_i - \\hat y_i)^2,$$\n",
    "where $\\hat y_i$ is the predicted value and $y_i$ is the true value of sample $i$. Together with a linear, non probabilistic model for the. For feature vector $x_i$ (where the first component is per convention the constant one -aka known as intercept) the prediction is given by the linear model. In scalar product notation:\n",
    "$$y_i =  x_i^T \\theta,$$\n",
    "with coefficient vector $\\theta$. Rewriting this in Matrix notation to account for all sampels we get for the loss together with the linear model:\n",
    "$$l = (Y - X\\theta)^T (Y - X\\theta)$$\n",
    "Now we would like to find $\\theta^*$ that _minimizes_ the loss. \n",
    "Setting the derivative w.r.t. $\\theta$ of the loss function zero  gives the normal equation \n",
    "$$X^T(Y - X\\theta) = 0.$$\n",
    "If $X^T X$ is non-singular (this is the case when there are more traning examples than features because then $X^T X$ is positive definite) allows for finding the unique solutions of the normal equation, \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "Note that this approach is non-probabilistic and thus, does not explicitly account for uncertainty  (as a probability distributions) in the data and coefficients.\n",
    "### Approach 2: Probabilistic  +  generative approach using max likelihood\n",
    "This approach introduces a probability distribution but does not explicitly consider a loss function. The  response is modelled  via a Normal distribution (\"Gauss error\") assuming constant standard deviation\n",
    "$$y_i = x_i^T \\theta + \\epsilon := \\mathcal N (x_i^T \\theta, \\sigma^2)$$\n",
    "In other words, the conditional distribtion  $p(y \\mid x, \\theta, \\sigma^2)$ is given by a Normal distribtion.\n",
    "The likelihood function is just the pdf of __all__ datapoints assuming i.i.d (this assumption in fact leads to the factorization), \n",
    "$$\\mathcal L = \\Pi_{i=1}^N p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "As we are aiming to optimize $\\theta$ in a way, a striclty monotonic transformation is applied on the likelihood function. It leaves the optimum invariant. The standard procedure is thus to consider the the logarithm ot likelihood function:\n",
    "$$\\mathcal L_l = \\sum_{i=1}^N \\log p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "Evaluating this expression for the Normal distribution gives\n",
    "$$\\mathcal L_l =  - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i - x_i^T \\theta)^2  - \\frac{N}{2}log(2\\pi\\sigma^2) $$\n",
    "Now, we would like to _maximise_ the likelihood and thus the log likelihood with respect to $\\theta$. This is equivalent to _minimizing_ the negative of it. Throwing away terms that, don't depend on $\\theta$ gives the function for which we would like to find the minimizer. That is we want to solve this expression, \n",
    "$$\\text{argmin}_\\theta\\left( Y - X \\theta \\right )^T \\left( Y - X \\theta \\right ),$$\n",
    "where we have rewritten the sum of squares over all training data again in Matrix notation. But this is exactly the same problem as in approach one and this gives the same solution (under the same circumstances), \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "**Remarks**\n",
    "* Note that this procedure only puts a probablilty distribution on the response $y$ while treating the remaining ingredients as variables (via the conditional pdf Ansatz). \n",
    "* This already implies that in this context the solutions $\\theta^*$ just tell how the pdf is parametrized (not even completely as we did not consider the optimum value of $\\sigma$).\n",
    "* In either case this approach does not really tell us how to predict a specific value $y$ for a given $x$. It just tells us the corresponding distribution of $y$. The fundamental reason is that we did not make any use of a loss function in this approach. \n",
    "* Pragmatically and in practice  of course, the prediction is made by plugging into the linear model as e.g. in the first appraoch\n",
    "* Note that a constant value for $\\sigma$ is called homoscedasticity. This implies that the variance does may not be a function of the features but only the mean within the Normal model for the response.\n",
    "* Furhtermore note that this approach does not consider uncertainties in the paramters. This would eventually require a Bayesian approach. \n",
    "\n",
    "### Approach 3: Probabilistic  +  discriminative approach using loss function and max likelihood\n",
    "\n",
    "### Apporach 4: A Bayesian approach\n",
    "\n",
    "Approaches: \n",
    "- Via max likelihood https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression\n",
    "- Minimize quadratic error directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression via the Normal equation - with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "supervised = namedtuple(\"supervised\", [\"features\", \"target\"])\n",
    "\n",
    "\n",
    "def split_test_train(data):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data.features, data.target, test_size = 0.2, random_state=5)\n",
    "    return supervised(X_train, Y_train.reshape(-1, 1)), supervised(X_test, Y_test.reshape(-1, 1))\n",
    "\n",
    "def add_intercept(features):\n",
    "    \"\"\"Add intercept to features\n",
    "    Todo: as an exercise use tensorflow\"\"\"\n",
    "    m, n = features.shape\n",
    "    return np.c_[np.ones((m, 1)), features]\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "data = supervised(housing.data, housing.target)\n",
    "train, test = split_test_train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: using the Normal equation\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables + graph\n",
    "tf.reset_default_graph()\n",
    "X = tf.constant(add_intercept(test.features), dtype=tf.float32, name=\"X\")\n",
    "Y = tf.constant(test.target, dtype=tf.float32, name=\"Y\")\n",
    "XT = tf.transpose(X)\n",
    "inverse = tf.matrix_inverse(tf.matmul(XT, X))\n",
    "residual = tf.matmul(XT, Y)\n",
    "theta = tf.matmul(inverse, residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7246094e+01]\n",
      " [ 4.3319702e-01]\n",
      " [ 1.0255814e-02]\n",
      " [-9.6313477e-02]\n",
      " [ 5.3881836e-01]\n",
      " [-8.9779496e-06]\n",
      " [-1.3463497e-03]\n",
      " [-4.2761230e-01]\n",
      " [-4.3939209e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Intitalize varaibales an compute graph\n",
    "# Why does it also work without variable initialization?\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    theta_value = theta.eval()   \n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.constant(add_intercept(test.features), dtype=tf.float32, name=\"X\")\n",
    "theta = tf.constant(theta_value, dtype=tf.float32, name=\"theta\")\n",
    "prediction = tf.matmul(X, theta)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    prediction_values = prediction.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison perform linear regression with scikit learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_model = LinearRegression(fit_intercept=True, normalize=False).fit(train.features, train.target)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15, 7))\n",
    "ax[0].plot(test.target, prediction_values, \"o\", color=\"red\", alpha=.5)\n",
    "ax[1].plot(test.target, lin_model.predict(test.features), \"o\", color=\"blue\", alpha=.5)\n",
    "for i in [0,1]:\n",
    "    ax[i].plot([0,10], [0,10], \"--\", lw=2, color=\"black\")\n",
    "    ax[i].set_xlabel(\"True value\",  fontsize=15)\n",
    "    ax[i].set_ylim(0,5.3)\n",
    "    ax[i].set_xlim(0,5.3)\n",
    "ax[0].set_ylabel(\"Predicted value (Tensorflow)\", fontsize=15);\n",
    "ax[1].set_ylabel(\"Predicted value (Scikit)\",  fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using functions\n",
    "* The above code is ugly\n",
    "* Rather we would like to put the graph construction in (a) function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing variables via placeholders\n",
    "# Name scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression via an optimizier \n",
    "* Assume that we could not find the minimizer analytically. \n",
    "* Thus we wish to find the minimzer computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
