{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python to define computational graph, which is exectued in C++ Code.\n",
    "Due to the graphical structure TF is highly parallizeable.\n",
    "TF can compute gradients automatically.\n",
    "The main python API is very flexible at the cost of higher complexity.\n",
    "It comes with tensorboard: a visualization tool for the computational graph.\n",
    "\n",
    "### Tensor flow APIs for machine learning\n",
    "* `tensorflow.estimator`: API for predefined models (very simalar to scikit-learn)\n",
    "* `tensorflow.losses`: Losses\n",
    "* `tensorflow.metrics`: Metrics\n",
    "* `tensorflow.layes`: Neural network layes\n",
    "\n",
    "\n",
    "\n",
    "### High level APIs to Tensorflow\n",
    "* `tensorflow.contrib.learn`: API compatible with scikit-learn\n",
    "* `tensorflow.contrib.slim`: API to simplify computations\n",
    "* `tensorflow.contrib.keras`: Keras API\n",
    "\n",
    "### Rought Structure of Tensorflow\n",
    "In principle (as e.g., in Spark) a tensorflow computation consists of two steps:\n",
    "1. **Construction phase:**\n",
    " This parts specifies the computational graph. I.e., it specifies the model to compute\n",
    " In tensorflow we call the associated object _dataflow graph_ (or just graph). Any additional variable is just a node added to the graph. The individual nodes are **Tensors** \n",
    " and **operations**. Tensors don't carry any data. \n",
    " \n",
    "2. **Execution phase:**\n",
    "This part then runs the actual computation and returns results. \n",
    "In Tensorflow the associated object is called a _session_. The return dypes of a session are **numpy arrays**.\n",
    "### Todos / Brainstorming\n",
    "* How to implement fit predict cleanly? \n",
    "* Warnings with Jupyter notebooks\n",
    "* Visualization of the computational graph\n",
    "\n",
    "### Possible Examples\n",
    "* Logistic Regression\n",
    "* Linear Regression\n",
    "* Generalized linear models\n",
    "* Survival Analysis\n",
    "* Customer Livetime values\n",
    "* Decision Tree\n",
    "* Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the computational graph\n",
    "# Neither functions are evaluated nor variables are initialized\n",
    "# Not executed yet (just like a tranformation in spark)\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(5, name=\"y\")\n",
    "f = x*x*x + x + y +42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "# A Session 1) initializes the variables + computes the graph\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer) # Initialize Variables\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f) # Run Graph\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "# Convientient alternative via context\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing variables\n",
    "**TF introduces shortcuts for readability:**\n",
    "* `x.initializer.run()` is equivalent to `tf.get_default_session().run(x.initializer)`\n",
    "* `f.eval()` is equivalent to `tf.get_default_session().run(f)` \n",
    "\n",
    "**Globally initializing variables**\n",
    "* Instead of  initializing each  variable individually, this may be done globally.\n",
    "* By calling `tf.global_variables_initializer()`\n",
    "* So the above code may be written more compactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() # add init node\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually run init\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow for experiments (jupyter notebook)\n",
    "* There are several ways to:\n",
    "    - speed up experimenting with TF\n",
    "    - reduce boilerplates (executing sessions)\n",
    "    - these are an 1) interactive tf session or 2) to enable eager execution (get rid of session calls)\n",
    "### Interactive TF session\n",
    "* Useful in particular for jupyter notebooks\n",
    "* Don't forget to close the session afterwards (would be done automatically within the session context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager execution\n",
    "* [Tensor flow documentation](https://www.tensorflow.org/guide/eager)\n",
    "* Within this execution mode TF operates similar to numpy \n",
    "* I.e, the graph is directly evaluated and thus calling the execution phase immediatly\n",
    "* To this end we need to tell tensorflow\n",
    "    - `tf.enable_eager_execution()`\n",
    "    \n",
    "**However, this operational mode is still under development!**\n",
    "\n",
    "> Eager execution is not included in the latest release (version 1.4) of TensorFlow. To use it, you will need to build TensorFlow from source or install the nightly builds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently you need to build TF in order to enable eager execution\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "try:\n",
    "    tfe.enable_eager_execution()\n",
    "    x = tf.constant([[2., 3.]], dtype=tf.float32, name=\"x\")\n",
    "    m = tf.matmul(x, x)\n",
    "except:\n",
    "    print(\"Currently you need to build TF in order to enable eager execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs\n",
    "* So far we did not assign any explicit graph\n",
    "* The reason is that tensoflow introduces the so called **default graph**\n",
    "* Any variable specified get's added to the default graph\n",
    "* However, we may also introduce other graphs, apart from the default graph.\n",
    "\n",
    "#### Graphs within jupyter notebooks\n",
    "* As jupyter notebooks are used for experimenting and cells are executed various times, it is possible that the graph get's \"messed up\". \n",
    "* In order to clean the graph tensoflow allows to clean the graph by the command `tf.reset_default_graph()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is x1 in default graph?: True\n",
      "Is x2 is in my_graph?: True\n",
      "Is x2 is in default_graph?: False\n"
     ]
    }
   ],
   "source": [
    "# node is added to default graph\n",
    "x1 = tf.Variable(3, \"x1\")\n",
    "print(\"Is x1 in default graph?:\", x1.graph is tf.get_default_graph())\n",
    "\n",
    "# create a new graph object\n",
    "my_graph = tf.Graph()\n",
    "with my_graph.as_default():\n",
    "    x2 = tf.Variable(30, \"x2\")\n",
    "\n",
    "print(\"Is x2 is in my_graph?:\", x2.graph is my_graph) \n",
    "print(\"Is x2 is in default_graph?:\", x2.graph is tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is x3 is in default graph: False\n"
     ]
    }
   ],
   "source": [
    "x3 = tf.Variable(333, \"x3\") # add to default graph\n",
    "tf.reset_default_graph() # reset default graph\n",
    "print(\"Is x3 is in default graph:\", x3.graph is tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a graph in a session\n",
    "* Instanite a new graph\n",
    "* Bild new graph\n",
    "* Instantiate Session with the new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "new_graph = tf.Graph()\n",
    "\n",
    "with new_graph.as_default():\n",
    "    x = tf.constant(3) # use float.32 as default\n",
    "    y = tf.constant(4) # use float.32 as default\n",
    "\n",
    "with tf.Session(graph=new_graph) as sess:\n",
    "    print(sess.graph is new_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Lifecycle\n",
    "* All node _values_ are dropped between graph runs (within a session).\n",
    "* Node values are (intermediate) results\n",
    "* This implies that node values are not reused (as in spark) per default.\n",
    "* This leads to  inefficiencies if also intermediate results are of interest. \n",
    "* For each (intermediate) result output TF runs an individual graph run.\n",
    "* In order to avoid redundant evaluations, you need to ask tensorflow to **evaluate all interesting variables within a single graph run**.\n",
    "* This can be achieved by \n",
    "\n",
    "\n",
    "\n",
    "#### Additional Information from the textbook (A. Geron, _Hands on machine learning with scikit-learn and tensoflow_, 2017):\n",
    "\n",
    "> All node values are dropped between graph runs, except variable values, which are maintained by the\n",
    "session across graph runs (queues and readers also maintain some state, as we will see in Chapter 12). A\n",
    "variable starts its life when its initializer is run, and it ends when the session is closed.\n",
    "\n",
    "> In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (each session would have its own copy of every variable). In distributed TensorFlow (see Chapter 12), variable state is stored on the servers, not in the sessions, so multiple sessions can share the same variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "# Redundant graph evaluation: 2 graphs are evaluated\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # 10\n",
    "    print(z.eval()) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# one graph computation only\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val =  sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types\n",
    "\n",
    "### Principle\n",
    "* Tensor flow datatypes are just placeholders\n",
    "\n",
    "### Tensors:\n",
    "\n",
    "\n",
    "### Constants and variables\n",
    "* **A constant is immutable.**\n",
    "* **A variable is mutable.** Thus, variables may change their values (e.g. by assign) constants not!\n",
    "* When working with Variables, they need to be explicitly intitialized, via ` tf.global_variables_initializer`.\n",
    " \n",
    "### Casting datatypes\n",
    "* `tf.cast`\n",
    "\n",
    "### Broadcasting and reshaping\n",
    "* https://colab.research.google.com/notebooks/mlcc/creating_and_manipulating_tensors.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=tensors-colab&hl=de\n",
    "* Tensorflow follows numpy in broadcasting and reshaping\n",
    "* In principle this is an elementwise operation with compatible dimension blowups\n",
    "* \n",
    "\n",
    "### Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4]\n",
      " [3 3]]\n",
      "[3 0]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant([[3,4], [3, 3]], name=\"x\")\n",
    "y = tf.Variable([3,4])\n",
    "y = y.assign([3, 0])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(x.eval())\n",
    "    print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed! Cannot assign constant!\n",
      "[[3 4]\n",
      " [3 3]]\n"
     ]
    }
   ],
   "source": [
    "# Constants are immutable -> error\n",
    "tf.reset_default_graph()\n",
    "x = tf.constant([[3,4], [3, 3]], name=\"x\")\n",
    "try:\n",
    "    x = x.assign([[0,0],[0,0]])\n",
    "except:\n",
    "    print(\"Failed! Cannot assign constant!\")\n",
    "with tf.Session() as sess:\n",
    "    print(x.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementwise sum between shapes (2, 2) and ()\n",
      "[[2 2]\n",
      " [3 3]]\n",
      "Elementwise sum between shapes (2, 2) and (1,)\n",
      "[[2 2]\n",
      " [3 3]]\n",
      "Elementwise sum between shapes (2, 2) and (2,)\n",
      "[[2 3]\n",
      " [3 4]]\n",
      "Elementwise sum between shapes (2, 2) and (2, 2)\n",
      "[[2 3]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Braodcasting\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.constant([[1,1],[2,2]]) # default type is tf.int32\n",
    "y = tf.constant(1) \n",
    "x_times_y = x + y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Elementwise sum between shapes {0} and {1}\".format(x.shape, y.shape))\n",
    "    print(x_times_y.eval())\n",
    "    \n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.constant([[1,1],[2,2]]) # default type is tf.int32\n",
    "y = tf.constant([1]) \n",
    "x_times_y = x + y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Elementwise sum between shapes {0} and {1}\".format(x.shape, y.shape))\n",
    "    print(x_times_y.eval())\n",
    "\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "x = tf.constant([[1,1],[2,2]]) # default type is tf.int32\n",
    "y = tf.constant([1,2]) \n",
    "x_times_y = x + y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Elementwise sum between shapes {0} and {1}\".format(x.shape, y.shape))\n",
    "    print(x_times_y.eval())    \n",
    "    \n",
    "tf.reset_default_graph()\n",
    "x = tf.constant([[1,1],[2,2]]) # default type is tf.int32\n",
    "y = tf.constant([[1,2], [3,4]]) \n",
    "x_times_y = x + y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Elementwise sum between shapes {0} and {1}\".format(x.shape, y.shape))\n",
    "    print(x_times_y.eval())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 2 and 3 for 'add' (op: 'Add') with input shapes: [2,2], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 2 and 3 for 'add' (op: 'Add') with input shapes: [2,2], [3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ca1d44e32632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# default type is tf.int32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_times_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    892\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 183\u001b[0;31m         \"Add\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 2 and 3 for 'add' (op: 'Add') with input shapes: [2,2], [3]."
     ]
    }
   ],
   "source": [
    "# Example with incompatible shape -> Throws error\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant([[1,1],[2,2]]) # default type is tf.int32\n",
    "y = tf.constant([1,2,4]) \n",
    "x_times_y = x + y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Elementwise sum between shapes {0} and {1}\".format(x.shape, y.shape))\n",
    "    print(x_times_y.eval())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]\n",
      " [13 14]\n",
      " [15 16]]\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]]\n",
      "\n",
      " [[ 9 10 11 12]\n",
      "  [13 14 15 16]]]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "matrix = tf.constant(\n",
    "    [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]],\n",
    "    dtype=tf.int32)\n",
    "\n",
    "reshaped_2x2x4_tensor = tf.reshape(matrix, [2, 2, 4])\n",
    "one_dimensional_vector = tf.reshape(matrix, [16])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(matrix.eval())\n",
    "    print(reshaped_2x2x4_tensor.eval())\n",
    "    print(one_dimensional_vector.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Feeding data into Graphs:](https://www.tensorflow.org/guide/low_level_intro)\n",
    "* There are two possibilietes of feeding training data into graphs:  via placeholders and \n",
    "datasets\n",
    "* Furthermore layers are objects used for trainable (mutable) model parameters\n",
    "\n",
    "### Placeholders:\n",
    "* `tf.placeholder`\n",
    "* The values are inserted by dictionary, `feed_dict` into the run method within a Session \n",
    "* Placeholders throw an error if no value is fed to them\n",
    "\n",
    "### Data sets:\n",
    "* [Doku for importing data with Datasets](https://www.tensorflow.org/guide/datasets)\n",
    "* Datasets are used for streaming data into the model\n",
    "* (Placeholder are good for experimenting)\n",
    "* To get a runable / iterable Tensor from Data two steps must be done\n",
    "    1. **Convert the data** to a `tf.data.Iterator` object (aka Dataset)\n",
    "    2. **Call** `tf.data.Iterator.get_next()` method\n",
    "    3. **Catch exception:** reaching the end of the dataset streaming an `tf.errors.OutofRangeError` is thrown\n",
    "* The easiest generator can be obtained with the  `tf.data.Dataset.make_one_shot_iterator`\n",
    "* The dataset may depend on stateful operations. In this case, it needs to be initialized before\n",
    "\n",
    "\n",
    "### Layers:\n",
    "* While placeholders and datasets are used for providing training data, layers are used for the the model parameters, because\n",
    "* in an (iteratively) trainable model, model paramters must be able to change during training / optimization.\n",
    "* In tensorflow the preferable way to do this is with the `tf.layers` object.\n",
    "* While this wording is motivated from neural networks it is a general principle\n",
    "* As varibles, **layers need to be intitialized** (`tf.initialize_global_variables()`)\n",
    "* **Shortcuts:**\n",
    "    - `tf.layers.Dense` returns an object that needs to be called on tensor\n",
    "    - `tf.layers.dense` is a method that takes input tensor and params from above init\n",
    "    - Difference in semantics: capital vs non capital letters\n",
    "    \n",
    "    \n",
    "### Shared variables:\n",
    "* **Todo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = x + y\n",
    "\n",
    "# placeholders are no varibles - no need for initialization\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z, feed_dict={x: 3, y: 4.5}))\n",
    "    print(sess.run(x, feed_dict={x: 10}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5\n"
     ]
    }
   ],
   "source": [
    "# Combining variables and placeholders\n",
    "tf.reset_default_graph()\n",
    "a = tf.Variable(42.0, tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = a + x + y\n",
    "\n",
    "# As we are having a variable here we need to initialize \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(sess.run(z, feed_dict={x: 3, y: 4.5}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion:  0\n",
      "[0 1]\n",
      "Portion:  1\n",
      "[2 3]\n",
      "Portion:  2\n",
      "[4 5]\n",
      "Portion:  3\n",
      "[6 7]\n",
      "Portion:  4\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "data = [\n",
    "    [0, 1,],\n",
    "    [2, 3,],\n",
    "    [4, 5,],\n",
    "    [6, 7,]]\n",
    "\n",
    "slices = tf.data.Dataset.from_tensor_slices(data) # 4 Tensors of shape 2\n",
    "iterator = slices.make_one_shot_iterator() # initialize iterator\n",
    "next_item = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Portion: \", i )\n",
    "            print(sess.run(next_item))\n",
    "            i = i + 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "# Layers\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3]) # 3 Features, N training samples as None\n",
    "# Initialize linear regression, initialize with ones for reproduction\n",
    "linear_model = tf.layers.Dense(units=1, \n",
    "                               activation=None, \n",
    "                               use_bias=True, \n",
    "                               kernel_initializer = tf.ones_initializer(),\n",
    "                               bias_initializer=tf.ones_initializer())\n",
    "# throw x on linear model\n",
    "y = linear_model(x)          \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}))\n",
    "    \n",
    "    \n",
    "# Remark: we set all model parameters to one. \n",
    "# So this is rather a prediction (for paremeters all one)\n",
    "# The power of layes comes when we train using an optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers using shortcut\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3]) \n",
    "# Initialize linear regression, initialize with ones for reproduction\n",
    "y = tf.layers.dense(x, \n",
    "                    units=1, \n",
    "                    activation=None, \n",
    "                    use_bias=True, \n",
    "                    kernel_initializer = tf.ones_initializer(),\n",
    "                    bias_initializer=tf.ones_initializer())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "* In principle losses can be defined manually using tensor datatypes. \n",
    "* However as it is so common to use losses in order to train a model via optimization (reducing loss), tensor flow provides the common loss functions\n",
    "* [`tf.losses`](https://www.tensorflow.org/api_docs/python/tf/losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared loss function for regression\n",
    "tf.reset_default_graph()\n",
    "y_true = tf.constant([1, 1, 1])\n",
    "y_pred = tf.constant([2, 2, 2])\n",
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "print(\"Loss is not a tensor but an operation!:\", type(loss), \"\\n\\n\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "* Tensor flow strongly follows the neural nets paradigm\n",
    "- Provide training data (`tf.data` or `tf.placeholder`)\n",
    "- Set up model (`tf.layers`)\n",
    "- Set up loss (`tf.losses`)\n",
    "- Find model parameters by optimizing the loss evaluated on training data\n",
    "\n",
    "* So the last ingredient are optimizers!\n",
    "- `tf.train`\n",
    "- The simplest optimizer one can imagine is gradient descent: `tf.train.GradientDescentOptimizer`\n",
    "- They have to be instantiated and thrown on a loss function (that in turn depends on input data).\n",
    "\n",
    "~~~~(.python)\n",
    "loss = ??? \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First simple example: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TF docu!\n",
    "# Data\n",
    "x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)\n",
    "y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)\n",
    "\n",
    "# Model\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "y_pred = linear_model(x)\n",
    "\n",
    "#Loss\n",
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "\n",
    "#Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(20): # optimization steps\n",
    "        _, loss_value = sess.run((train, loss))\n",
    "        print(loss_value)\n",
    "    print(sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - some Theory\n",
    "For linear regression the closed solution is known via the so called normal equation\n",
    "Todo:\n",
    "1. Specify model\n",
    "2. Speficy loss function\n",
    "2. Write down log Likelihood function\n",
    "3. Minimize + get Normal equation\n",
    "\n",
    "### Approach 1: Non-probabilisitc using loss function\n",
    "The easies approach to linear regression is to use the squared loss function, \n",
    "$$l = \\sum_{i=1}^N (y_i - \\hat y_i)^2,$$\n",
    "where $\\hat y_i$ is the predicted value and $y_i$ is the true value of sample $i$. Together with a linear, non probabilistic model for the. For feature vector $x_i$ (where the first component is per convention the constant one -aka known as intercept) the prediction is given by the linear model. In scalar product notation:\n",
    "$$y_i =  x_i^T \\theta,$$\n",
    "with coefficient vector $\\theta$. Rewriting this in Matrix notation to account for all sampels we get for the loss together with the linear model:\n",
    "$$l = (Y - X\\theta)^T (Y - X\\theta)$$\n",
    "Now we would like to find $\\theta^*$ that _minimizes_ the loss. \n",
    "Setting the derivative w.r.t. $\\theta$ of the loss function zero  gives the normal equation \n",
    "$$X^T(Y - X\\theta) = 0.$$\n",
    "If $X^T X$ is non-singular (this is the case when there are more traning examples than features because then $X^T X$ is positive definite) allows for finding the unique solutions of the normal equation, \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "Note that this approach is non-probabilistic and thus, does not explicitly account for uncertainty  (as a probability distributions) in the data and coefficients.\n",
    "### Approach 2: Probabilistic  +  generative approach using max likelihood\n",
    "This approach introduces a probability distribution but does not explicitly consider a loss function. The  response is modelled  via a Normal distribution (\"Gauss error\") assuming constant standard deviation\n",
    "$$y_i = x_i^T \\theta + \\epsilon := \\mathcal N (x_i^T \\theta, \\sigma^2)$$\n",
    "In other words, the conditional distribtion  $p(y \\mid x, \\theta, \\sigma^2)$ is given by a Normal distribtion.\n",
    "The likelihood function is just the pdf of __all__ datapoints assuming i.i.d (this assumption in fact leads to the factorization), \n",
    "$$\\mathcal L = \\Pi_{i=1}^N p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "As we are aiming to optimize $\\theta$ in a way, a striclty monotonic transformation is applied on the likelihood function. It leaves the optimum invariant. The standard procedure is thus to consider the the logarithm ot likelihood function:\n",
    "$$\\mathcal L_l = \\sum_{i=1}^N \\log p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "Evaluating this expression for the Normal distribution gives\n",
    "$$\\mathcal L_l =  - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i - x_i^T \\theta)^2  - \\frac{N}{2}log(2\\pi\\sigma^2) $$\n",
    "Now, we would like to _maximise_ the likelihood and thus the log likelihood with respect to $\\theta$. This is equivalent to _minimizing_ the negative of it. Throwing away terms that, don't depend on $\\theta$ gives the function for which we would like to find the minimizer. That is we want to solve this expression, \n",
    "$$\\text{argmin}_\\theta\\left( Y - X \\theta \\right )^T \\left( Y - X \\theta \\right ),$$\n",
    "where we have rewritten the sum of squares over all training data again in Matrix notation. But this is exactly the same problem as in approach one and this gives the same solution (under the same circumstances), \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "**Remarks**\n",
    "* Note that this procedure only puts a probablilty distribution on the response $y$ while treating the remaining ingredients as variables (via the conditional pdf Ansatz). \n",
    "* This already implies that in this context the solutions $\\theta^*$ just tell how the pdf is parametrized (not even completely as we did not consider the optimum value of $\\sigma$).\n",
    "* In either case this approach does not really tell us how to predict a specific value $y$ for a given $x$. It just tells us the corresponding distribution of $y$. The fundamental reason is that we did not make any use of a loss function in this approach. \n",
    "* Pragmatically and in practice  of course, the prediction is made by plugging into the linear model as e.g. in the first appraoch\n",
    "* Note that a constant value for $\\sigma$ is called homoscedasticity. This implies that the variance does may not be a function of the features but only the mean within the Normal model for the response.\n",
    "* Furhtermore note that this approach does not consider uncertainties in the paramters. This would eventually require a Bayesian approach. \n",
    "\n",
    "### Approach 3: Probabilistic  +  discriminative approach using loss function and max likelihood\n",
    "\n",
    "### Apporach 4: A Bayesian approach\n",
    "\n",
    "Approaches: \n",
    "- Via max likelihood https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression\n",
    "- Minimize quadratic error directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression via the Normal equation - with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "supervised = namedtuple(\"supervised\", [\"features\", \"target\"])\n",
    "\n",
    "\n",
    "def split_test_train(data):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data.features, data.target, test_size = 0.2, random_state=5)\n",
    "    return supervised(X_train, Y_train.reshape(-1, 1)), supervised(X_test, Y_test.reshape(-1, 1))\n",
    "\n",
    "def add_intercept(features):\n",
    "    \"\"\"Add intercept to features\n",
    "    Todo: as an exercise use tensorflow\"\"\"\n",
    "    m, n = features.shape\n",
    "    return np.c_[np.ones((m, 1)), features]\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "data = supervised(housing.data, housing.target)\n",
    "train, test = split_test_train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: using the Normal equation\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables + graph\n",
    "tf.reset_default_graph()\n",
    "X = tf.constant(add_intercept(test.features), dtype=tf.float64, name=\"X\")\n",
    "Y = tf.constant(test.target, dtype=tf.float64, name=\"Y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitalize varaibales an compute graph\n",
    "# Why does it also work without variable initialization?\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    theta_value = theta.eval()   \n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.constant(add_intercept(test.features), dtype=tf.float64, name=\"X\")\n",
    "theta = tf.constant(theta_value, dtype=tf.float64, name=\"theta\")\n",
    "prediction = tf.matmul(X, theta)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    prediction_values = prediction.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison perform linear regression with scikit learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_model = LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_model.fit(train.features, train.target)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15, 7))\n",
    "ax[0].plot(test.target, prediction_values, \"o\", color=\"red\", alpha=.5)\n",
    "ax[1].plot(test.target, lin_model.predict(test.features), \"o\", color=\"blue\", alpha=.5)\n",
    "for i in [0,1]:\n",
    "    ax[i].plot([0,10], [0,10], \"--\", lw=2, color=\"black\")\n",
    "    ax[i].set_xlabel(\"True value\",  fontsize=15)\n",
    "    ax[i].set_ylim(0,5.3)\n",
    "    ax[i].set_xlim(0,5.3)\n",
    "ax[0].set_ylabel(\"Predicted value (Tensorflow)\", fontsize=15);\n",
    "ax[1].set_ylabel(\"Predicted value (Scikit)\",  fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE on training set:\n",
    "tf.reset_default_graph()\n",
    "X = tf.constant(add_intercept(train.features), dtype=tf.float64, name=\"X\")\n",
    "theta = tf.constant(theta_value, dtype=tf.float64, name=\"theta\")\n",
    "prediction = tf.matmul(X, theta)\n",
    "y_true = tf.constant(train.target, dtype=tf.float64, name=\"true_label\")\n",
    "y_pred_scikit = tf.constant(lin_model.predict(train.features), dtype=tf.float64, name=\"prediction_label\")\n",
    "mse = tf.reduce_mean(tf.square(y_true - prediction))\n",
    "mse_scikit = tf.reduce_mean(tf.square(y_true - y_pred_scikit))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Tensorflow MSE on training data:\",  mse.eval())\n",
    "    print(\"Scikit Learn MSE on training data:\", mse_scikit.eval())\n",
    "    \n",
    "    \n",
    "# MSE on test set: \n",
    "tf.reset_default_graph()\n",
    "y_true = tf.constant(test.target, dtype=tf.float64)\n",
    "y_pred_scikit = tf.constant(lin_model.predict(test.features), dtype=tf.float64)\n",
    "y_pred = tf.constant(prediction_values, dtype=tf.float64)\n",
    "mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "mse_scikit = tf.reduce_mean(tf.square(y_true - y_pred_scikit))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Tensorflow MSE on test data:\",  mse.eval())\n",
    "    print(\"Scikit Learn MSE on test data:\", mse_scikit.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using functions\n",
    "* The above code is ugly\n",
    "* Rather we would like to put the graph construction in (a) function(s)\n",
    "\n",
    "# Linear Regression using the estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing variables via placeholders\n",
    "# Name scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression via an optimizier \n",
    "* Assume that we could not find the minimizer analytically. \n",
    "* Thus we wish to find the minimzer computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ops vs data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
