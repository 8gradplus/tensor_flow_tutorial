{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - some Theory\n",
    "* [other compariative exmaples](https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/)\n",
    "For linear regression the closed solution is known via the so called normal equation\n",
    "Todo:\n",
    "1. Specify model\n",
    "2. Speficy loss function\n",
    "2. Write down log Likelihood function\n",
    "3. Minimize + get Normal equation\n",
    "\n",
    "### Approach 1: Non-probabilisitc using loss function\n",
    "The easies approach to linear regression is to use the squared loss function, \n",
    "$$l = \\sum_{i=1}^N (y_i - \\hat y_i)^2,$$\n",
    "where $\\hat y_i$ is the predicted value and $y_i$ is the true value of sample $i$. Together with a linear, non probabilistic model for the. For feature vector $x_i$ (where the first component is per convention the constant one -aka known as intercept) the prediction is given by the linear model. In scalar product notation:\n",
    "$$y_i =  x_i^T \\theta,$$\n",
    "with coefficient vector $\\theta$. Rewriting this in Matrix notation to account for all sampels we get for the loss together with the linear model:\n",
    "$$l = (Y - X\\theta)^T (Y - X\\theta)$$\n",
    "Now we would like to find $\\theta^*$ that _minimizes_ the loss. \n",
    "Setting the derivative w.r.t. $\\theta$ of the loss function zero  gives the normal equation \n",
    "$$X^T(Y - X\\theta) = 0.$$\n",
    "If $X^T X$ is non-singular (this is the case when there are more traning examples than features because then $X^T X$ is positive definite) allows for finding the unique solutions of the normal equation, \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "Note that this approach is non-probabilistic and thus, does not explicitly account for uncertainty  (as a probability distributions) in the data and coefficients.\n",
    "### Approach 2: Probabilistic  +  generative approach using max likelihood\n",
    "This approach introduces a probability distribution but does not explicitly consider a loss function. The  response is modelled  via a Normal distribution (\"Gauss error\") assuming constant standard deviation\n",
    "$$y_i = x_i^T \\theta + \\epsilon := \\mathcal N (x_i^T \\theta, \\sigma^2)$$\n",
    "In other words, the conditional distribtion  $p(y \\mid x, \\theta, \\sigma^2)$ is given by a Normal distribtion.\n",
    "The likelihood function is just the pdf of __all__ datapoints assuming i.i.d (this assumption in fact leads to the factorization), \n",
    "$$\\mathcal L = \\Pi_{i=1}^N p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "As we are aiming to optimize $\\theta$ in a way, a striclty monotonic transformation is applied on the likelihood function. It leaves the optimum invariant. The standard procedure is thus to consider the the logarithm ot likelihood function:\n",
    "$$\\mathcal L_l = \\sum_{i=1}^N \\log p(y_i \\mid x, \\theta, \\sigma).$$\n",
    "Evaluating this expression for the Normal distribution gives\n",
    "$$\\mathcal L_l =  - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i - x_i^T \\theta)^2  - \\frac{N}{2}log(2\\pi\\sigma^2) $$\n",
    "Now, we would like to _maximise_ the likelihood and thus the log likelihood with respect to $\\theta$. This is equivalent to _minimizing_ the negative of it. Throwing away terms that, don't depend on $\\theta$ gives the function for which we would like to find the minimizer. That is we want to solve this expression, \n",
    "$$\\text{argmin}_\\theta\\left( Y - X \\theta \\right )^T \\left( Y - X \\theta \\right ),$$\n",
    "where we have rewritten the sum of squares over all training data again in Matrix notation. But this is exactly the same problem as in approach one and this gives the same solution (under the same circumstances), \n",
    "\n",
    "$$\\theta ^* =  \\left( X^T X  \\right)^{-1} X^T Y$$\n",
    "\n",
    "**Remarks**\n",
    "* Note that this procedure only puts a probablilty distribution on the response $y$ while treating the remaining ingredients as variables (via the conditional pdf Ansatz). \n",
    "* This already implies that in this context the solutions $\\theta^*$ just tell how the pdf is parametrized (not even completely as we did not consider the optimum value of $\\sigma$).\n",
    "* In either case this approach does not really tell us how to predict a specific value $y$ for a given $x$. It just tells us the corresponding distribution of $y$. The fundamental reason is that we did not make any use of a loss function in this approach. \n",
    "* Pragmatically and in practice  of course, the prediction is made by plugging into the linear model as e.g. in the first appraoch\n",
    "* Note that a constant value for $\\sigma$ is called homoscedasticity. This implies that the variance does may not be a function of the features but only the mean within the Normal model for the response.\n",
    "* Furhtermore note that this approach does not consider uncertainties in the paramters. This would eventually require a Bayesian approach. \n",
    "\n",
    "\n",
    "### Apporach 4: Uncorrelated Bayesian Approach\n",
    "A Bayesian linar regression is a straightforward generalization, \n",
    "#### The model:\n",
    "$$y_i = x_i^T \\theta + \\epsilon \\propto \\mathcal N (y_i  | x_i^T \\theta, \\sigma^2)\\\\ \n",
    "p(\\theta)  \\propto \\mathcal N (\\theta | 0, \\lambda ^{-1} \\mathbb 1 ) \n",
    "$$\n",
    "\n",
    "* This model takes as input parameters $\\lambda$ and $\\sigma^2$.\n",
    "* The prio of $\\theta$ is a Normal distribution parametrized by the parameters $\\lambda$\n",
    "* This lends also the model its name as the prior is uncorrelated (i.e., diagonal $\\Sigma$ Matrix)\n",
    "* **Todo: in order to make contact with the non-baysian formulations above we need to $X \\rightarrow X^T$**\n",
    "\n",
    "#### Learning\n",
    "*  Learning corresponds to determining the posterior probability distribution of $\\theta$: \n",
    "$$\\mathcal N \\left(\\theta ~ |~ \\left(XX^T + \\lambda \\sigma^2 \\mathbb 1\\right)XY, \\left( \\lambda \\mathbb 1 + \\frac{1}{\\sigma^2} XX^T\\right)^{-1} \\right)$$\n",
    "in other words posterior of $\\theta$ follows a normal distribution with \n",
    "- **mean**\n",
    "$$\\left(XX^T + \\lambda \\sigma^2 \\mathbb 1\\right)XY$$\n",
    "- **standard deviation**\n",
    "$$\\left( \\lambda \\mathbb 1 + \\frac{1}{\\sigma^2} XX^T\\right)^{-1}$$\n",
    "* Note the feature correlation term the makes the posterior correlated!\n",
    "\n",
    "#### Inference\n",
    "* Corresponds to predicting a new value $y^*$ for a new $x ^*$\n",
    "$$p(y^* ~|~ x^* \\sigma^2 \\lambda, \\mathcal D) = \n",
    "\\mathcal N \\left( y^* ~|~ w^Tx^*, ~  \\sigma^2  + x^*\\left(\\lambda \\mathbb1 + \\frac{1}{\\sigma^2} XX^T\\right)^{-1}x^{*T}\\right)$$\n",
    "\n",
    "#### Ridge Regression as limiting case\n",
    "* Learning\n",
    "* $\\lambda \\sigma ^2 \\rightarrow \\lambda$ (regularization parameter)\n",
    "* $\\langle p(\\theta ~|~ \\mathcal D, \\lambda, \\sigma^2) \\rangle =$ Estimator of ridge regression\n",
    "* Prediction\n",
    "* $\\langle p(y^* ~|~ x^* \\sigma^2 \\lambda, \\mathcal D)\\rangle = w^T x^*$ (= Ridge Regression prediction)\n",
    "\n",
    "\n",
    "\n",
    "### Remarks\n",
    "Approaches: \n",
    "- Via max likelihood https://www.quantstart.com/articles/Maximum-Likelihood-Estimation-for-Linear-Regression\n",
    "- Minimize quadratic error directly\n",
    "\n",
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression via the Normal equation - with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Tensorflow / Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the estimator API\n",
    "* within the estimator API the number of training steps is controlled via the the `dataset`\n",
    "* more precisely via the `repeat` keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
